\chapter{Literature Review}
% what is going on in this chapter (0,25p)
This chapter presents the current state of research in two different domains. The first is about predicting sporting events using machine learning. The latter examines sports betting with a particular focus on betting odds and how these can help to predict events in the future. \\
\indent The established guidelines of \citet{vom_brocke_standing_2015} and \citet{webster_guest_2002}, are used to determine the current state of research and respectively document the literature search process. As stated by \citet{webster_guest_2002}, two types of literature reviews exist. This literature review belongs to the second type, which is, according to \citeauthor{webster_guest_2002}, in general, shorter and where \emph{'authors [...] tackle an emerging issue that would benefit from exposure to potential theoretical foundations'} \parencite[, p. 14]{webster_guest_2002}. First, as recommended by \citet{vom_brocke_standing_2015}, the literature search process is documented as accurately as possible to facilitate future research on this topic. Then, the literature found is summarised in a concept matrix according to \citet{webster_guest_2002} and examined according to specially selected criteria. Based on this examination, research gaps are identified, and finally, the research question for this thesis is formulated.

% describing the search process
According to \citet{vom_brocke_standing_2015}, in order to find relevant literature on the research areas dealt with, the topic is divided into separate concepts. These concepts help to find literature in scholarly databases using keyword search. The keywords searched for in this thesis were \emph{'fantasy football'}, \emph{'machine learning'}, \emph{'prediction'} and \emph{'betting odds'}. The keywords were entered in every existing combination to find articles that do not correspond to all keywords. Based on the research of \citet{gusenbauer_google_2019}, \emph{Google Scholar} and \emph{Microsoft Academic}, the most extensive academic search engines, were used for the literature search. When selecting the results from this search, attention is paid to the currently awarded VHB journal rankings \parencite[see][]{vhb_e_v_vhb-jourqual3_2015} for the sub-field of business informatics to ensure that the literature researched is of high quality. This ranking is chosen because it is well-known and accepted in the German research area. One journal that would be less considered following this ranking, but seems extremely relevant to the research in this thesis, is the \emph{Journal of Quantitative Analysis in Sports} (JQAS). This journal gets published by the American Statistical Association (ASA), which according to themselves, \emph{'is the world's largest community of statisticians'} \parencite[see][]{noauthor_about_nodate}. Using the papers from the JQAS and journals highly ranked by the VHB, the remaining literature is found using backward search and forward search suggested by \citet{webster_guest_2002}.

% explaining the concept matrix and the criteria (1p)
In the process mentioned above, 22 papers were examined and compared in a concept matrix (see Figure \ref{tab:concept_matrix} on page \pageref{tab:concept_matrix}) as required by \citet{webster_guest_2002}. The concepts used to examine the papers will be briefly discussed from left to right in this paragraph. \\
\indent The year of publication, the VHB ranking and the distinction in which form the paper was published serve to evaluate the quality of the literature. That is to ensure that primarily the most recent papers in renowned peer-reviewed journals were analysed. The sport discipline helps to notice similar approaches in different sports. While sports differ, some are more related than others. The main idea behind this is that there may be viable approaches from a similar sport that would have been unconsidered otherwise.  \\
\indent During the research, to the best of my knowledge, no publication was found which deals precisely with the problem at hand. For this reason, the research had to focus on similar approaches, objectives or tasks. The solving approaches vary from more straightforward approaches such as mixed integer programming to more complex multi-hierarchical Bayesian models. Some publications used a combination of several methodologies, which are strongly dependent on the task to be solved. A distinction was therefore made between optimisation and prediction tasks. Although almost all papers unanimously had the goal of setting up a team that would score as many points as possible, they came at the solution differently. The matrix distinguishes between publications that optimised only the team performance as a whole and those that predicted the performance for each individual player and then combined the best players into a team. At the same time, it investigated which papers relied on betting odds or another form of prediction markets. Lastly, the data used in each publication was analysed. Due to the always different data, a generalised view was applied, which examines whether time-series data is used, whether the home advantage was taken into account and whether betting odds were used. \\
\indent The articles are sorted by criteria in the following order: \emph{'VBA Rank', 'Machine Learning Approach', 'Neural Network', 'Individual Performance', 'Betting Odds'}. This sorting ensures that the papers that are most similar to the thesis at hand and at the same time have a high VBA Rank are displayed first. For comparison purposes, the thesis at hand can be found at the bottom of the matrix. In this way, it can be quickly recognised that no publication deals precisely with the problem of the thesis. The paper that is closest to the topic is the paper by \citet{landers_machine_2017}, even if it investigates football instead of soccer.

% conclude the concepts from the papers (3p)
% In this way, it can be quickly recognised that no publication deals precisely with the problem of the thesis. The paper that is closest to the topic is the paper by \citet{landers_machine_2019}, even if it investigates football instead of soccer. In their work, \citeauthor{landers_machine_2019} divide the problem into two sub-areas. The first area is predicting individual player performances using machine learning - but without the influence of betting-specific metrics. The second area deals with betting against the spread, where the supposedly worse team is credited with points or goals in advance, which are then added to the points scored in the end. This type of game can be compared to betting odds, as in both cases, it predicts which team is likely to win. \citeauthor{landers_machine_2019} treat these two areas separately, each with a different set of data. In both cases, they use \emph{Gradient Boosted Decision Trees} and thus outperform other players, especially for area 2. One feature that is discussed in many of the papers examined is the home advantage. Although according to the results of \citet{nevill_home_1999} the home advantage does exist, \citeauthor{landers_machine_2019} state that \emph{'the home-field advantage does not appear to be of major significance [...]'} \parencite[, p. 5]{landers_machine_2019} in their research. This contrasts with the research of \citet{deng_analysis_2020}, whose work has a strong focus on home advantage through the separation in home and away performances. In their publication, they compare four models according to their accuracy: \emph{Deep Neural Network (0.99), Logistic Regression (0.95), Decision Trees (0.91)} and \emph{Random Forest (0.84)}. All models achieve an accuracy of over 84\%, though the DNN might be overfitted with a 99\% accuracy. \citeauthor{deng_analysis_2020} are also the only ones in the papers studied who used betting odds to predict performance, even if they only predict team results instead of individual performances. 

The following paragraph summarises various concepts that have been frequently discussed in the presented literature. These topics are presented in alignment with the data mining process.  \\
% \indent The first concept discussed is the \emph{baseline}. In order to find out how well a model fits the task, a suitable measure must be found with which the models can be compared. In their work, \citeauthor{landers_machine_2019} establish two kinds of baselines. The first one is the minimum of points one player needs to score to break even with the fee it costs to put him in the lineup. They use this threshold to find out how successfully a whole team performs. This is an a posteriori approach because the difference between the points value obtained and the threshold can only be calculated afterwards. Furthermore, \citet{landers_machine_2019} propose an a priori baseline - the perceptron algorithm. \emph{'The perceptron algorithm represents an early version
% of modern neural networks and is used in this context as a
% baseline linear classifier to be compared with gradient boosted
% decision trees[...]'} \parencite[, p. 6]{landers_machine_2019}  
\indent The first concept discussed is the \emph{preprocessing of the data}. In order to achieve optimal results, the data mining process must adjust the data in advance without compromising its validity. Many of the authors tackle the problem that few exceptional players outperform the average players. These outliers are firstly hard to predict and secondly degrade the prediction accuracy. To solve this problem, the authors used various techniques to boost their prediction accuracy. For example, \citet{landers_machine_2017} developed a calculated threshold that players must reach at least to be included in the analysis. All players below this threshold are sorted out. At this point, it should be mentioned that it is crucial to choose a threshold instead of a point range, as in this case, the players with the highest points are not omitted. This approach can only be applied if data from previous games are available. \citet{lutz_fantasy_2015,egidi_bayesian_2018,yurko_nflwar_2019} focus in their papers on what to do if this data is not available. \citet{yurko_nflwar_2019} state that one major problem they could not solve that negatively influences the team performance is the uncertainty of players appearing in the lineup due to unpredictable events with no evidential data like injuries. \citet{lutz_fantasy_2015} investigates the case of new players who joined at the beginning of the season ('new joiners'), as these players naturally do not have previous game data. He suggests taking the mean points of all players on a similar position in this case. \parencite[cf.][, p. 3]{lutz_fantasy_2015} In contrast to that, \citet{egidi_bayesian_2018} take a different approach. In their paper, they compare two different solutions to this problem. On the first try, they put the expected points to zero, and on the second try, they guess the points in a calculated range. In both cases, the processed points from the player often were too low to be considered for their starting lineup. Nevertheless, they find out that the second approach is more precise and improves their models overall. Furthermore, \citet{egidi_bayesian_2018} discover that simplifying the data, if more details do not add value, increase their models' accuracy as well. This is similar to the approach \citet{deng_analysis_2020} take. In their studies, they use the \emph{Kaggle European Soccer Database}, a table with a total of 144 attributes, wherefrom they only carefully select 28 attributes to improve the model. \\
\indent This links to the second concept, the \emph{feature selection}. As mentioned, \citet{deng_analysis_2020,egidi_bayesian_2018} reduce the attributes fed to their model to increase accuracy. In his paper, \citet{lutz_fantasy_2015} examines precisely the question of if and how far the number of attributes must be limited. He proceeds in three different ways. First, he does not exclude any features, figuring out that this approach is the least accurate. Secondly, he selects the features manually according to his assessment. Lastly, he chooses a more analytical path: \emph{Recursive Feature Elimination with Cross Validation} (RFECV). This method \emph{'recursively eliminates features and checks if the regression method's results improve by cross-validating.'} \parencite[, p. 4]{lutz_fantasy_2015} This calculated approach yields the highest prediction accuracy. This method in combination with \emph{univariate selection} was used by \citet{anik_players_2018} as well. One key feature that is discovered in this way is the position of the players. \citet{lutz_fantasy_2015,demediuk_performance_2021,egidi_bayesian_2018} all increased their accuracy by modeling each position separately. Similar to \citeauthor{lutz_fantasy_2015}' second manual approach, \citeauthor{deng_analysis_2020} also select their features based on their perception and note that \emph{'sufficient background knowledge of the practical application is essential.'} \parencite[, p. 4]{deng_analysis_2020}. That confirms the discovery of \citeauthor{rein_big_2016}, who claim that at the current state of research, \emph{'most [Machine Learning] soccer analyses are performed by computer scientist research group with little apparent involvement by sports scientists.'} \parencite[, p. 6]{rein_big_2016}. \\
\indent From these researches could be inferred that it is beneficial to interview sports experts on their opinion on essential features if manual feature selection is used. However, if this is not possible, feature selection algorithms should be applied. In addition, the models could be even further improved by omitting players and features that offer little added value for the predictions. Each position should thereby be modelled separately. Finally, missing data can be dealt with in three ways: from setting it to zero, giving it a mean value from similar players, and estimating it accurately. The latter is promising the most success.

Once the feature selection process is complete, the next step, respectively the third concept, is to select the right \emph{machine learning approach}. First, as in the concept matrix, a distinction must be made between optimisation and prediction tasks. Only two different methodologies were chosen for the optimisation task: either brute force optimisation \parencite{landers_machine_2017} or mixed-integer programming \parencite{becker_analytical_2016,edwards_analyzing_2018, belien_optimization_2017,bonomo_mathematical_2014,matthews_competing_2012}. Which of these two methods is used depends primarily on how much computing power is required for the previous task. \\
\indent For the prediction task, a variety of methods are used that range from simple linear regression to more complex feed-forward deep neural networks. In the following, the focus is limited to the three most frequently used and most promising methods:  \emph{Gradient Boosted Decision Trees} \parencite{landers_machine_2017,deng_analysis_2020}, \emph{Random Forest} \parencite{deng_analysis_2020,shah_poisson_2021,demediuk_performance_2021,bhateja_analysis_2021} and \emph{Deep Neural Networks} \parencite{bhateja_analysis_2021,skinner_method_2015,deng_analysis_2020,lutz_fantasy_2015,landers_machine_2017}. In their paper, \citet{deng_analysis_2020} 


% Concept-Matrix
\subimport{section/}{matrix.tex}

% give research gaps, even for future stuff WW
% Of all the publications analysed in the matrix, two publications are most similar to the problem at hand: \cite{deng_analysis_2020}, and \cite{landers_machine_2019}. Although Deng and Zhong write at the beginning of their article that they use time-series soccer data and bettings odds in their models, the bettings odds are not discussed in particular further in the rest of the article. Furthermore, the paper is in a journal not ranked by the VHB, which calls into question whether the publication is peer-reviewed. \citeauthor{landers_machine_2019} publication, on the contrary, is published in a journal of the IEEE. They divide their paper into two subsections - the first dealing with predicting individual player performances and the second comparing their forecasts against the betting spread. 

% explain chosen gap / research question (1p)
\textbf{Placeholder: explain chosen gap / research question }


% CONCEPT-MATRIX-STUFF
% \settowidth{\rotheadsize}{Publication}

% \begin{longtable}{lcccccccc}\toprule
%              &                  & \multicolumn{3}{c}{Published In} & \\ \cmidrule{3-5}
% Paper        & \rothead[c]{Publication\\Year} & \rothead[c]{Journal} & \rothead[c]{Conference\\Proceeding} & \rothead[c]{Other} & \rothead[c]{VBA-\\Ranking} & \rothead[c]{Discipline} \\ \midrule
% Tool Alpha   & NO               & OK      & a\\
% Tool Delta   & NO               & NO \\
% Tool Gamma   & NO               & OK \\
% Tool Theta   & NO               & NO \\
% Tool Upsilon & OK               & NO \\
% \textbf{Tool X (our proposal)} & OK &   OK \\ \bottomrule
% \end{longtable}Furthermore