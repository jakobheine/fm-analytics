\chapter{Literature Review}
\label{chap:literature_review}

% what is going on in this chapter (0,25p)
This chapter presents the current state of research in two different domains. The first is about predicting sporting events using machine learning. The latter examines sports betting with a particular focus on betting odds and how these can help to predict events in the future. \\
\indent The established guidelines of \citet{vom_brocke_standing_2015} and \citet{webster_guest_2002}, are used to determine the current state of research and respectively document the literature search process. As stated by \citet{webster_guest_2002}, two types of literature reviews exist. This literature review belongs to the second type, which is, according to \citeauthor{webster_guest_2002}, in general, shorter and where \emph{'authors [...] tackle an emerging issue that would benefit from exposure to potential theoretical foundations'} \parencite[, p. 14]{webster_guest_2002}. First, as recommended by \citet{vom_brocke_standing_2015}, the literature search process is documented as accurately as possible to facilitate future research on this topic. Then, the literature found is summarised in a concept matrix according to \citet{webster_guest_2002} and examined according to specially selected criteria. Based on this examination, research gaps are identified, and finally, the research question for this thesis is formulated.

% describing the search process
According to \citet{vom_brocke_standing_2015}, in order to find relevant literature on the research areas dealt with, the topic is divided into separate concepts. These concepts help to find literature in scholarly databases using keyword search. The keywords searched for in this thesis were \emph{'fantasy football'}, \emph{'machine learning'}, \emph{'prediction'} and \emph{'betting odds'}. The keywords were entered in every existing combination to find articles that do not correspond to all keywords. Based on the research of \citet{gusenbauer_google_2019}, \emph{Google Scholar} and \emph{Microsoft Academic}, the most extensive academic search engines, were used for the literature search. When selecting the results from this search, attention is paid to the currently awarded VHB journal rankings \parencite[see][]{vhb_e_v_vhb-jourqual3_2015} for the sub-field of business informatics to ensure that the literature researched is of high quality. This ranking is chosen because it is well-known and accepted in the German research area. One journal that would be less considered following this ranking, but seems extremely relevant to the research in this thesis, is the \emph{Journal of Quantitative Analysis in Sports} (JQAS). This journal gets published by the American Statistical Association (ASA), which according to themselves, \emph{'is the world's largest community of statisticians'} \parencite[see][]{noauthor_about_nodate}. Using the papers from the JQAS and journals highly ranked by the VHB, the remaining literature is found using backward search and forward search suggested by \citet{webster_guest_2002}.

% explaining the concept matrix and the criteria (1p)
In the process mentioned above, 22 papers were examined and compared in a concept matrix (see Figure \ref{tab:concept_matrix} on page \pageref{tab:concept_matrix}) as required by \citet{webster_guest_2002}. The concepts used to examine the papers will be briefly discussed from left to right in this paragraph. \\
\indent The year of publication, the VHB ranking, and the distinction in which form the paper was published serve to evaluate the quality of the literature. That is to ensure that primarily the most recent papers in renowned peer-reviewed journals were analyzed. The sport discipline helps to notice similar approaches in different sports. While sports differ, some are more related than others. The main idea behind this is that there may be viable approaches from a similar sport that would have been unconsidered otherwise.  \\
\indent During the research, to the best of my knowledge, no publication was found which deals precisely with the problem at hand. For this reason, the research had to focus on similar approaches, objectives, or tasks. The solving approaches vary from more straightforward approaches such as mixed integer programming to more complex multi-hierarchical Bayesian models. Some publications used a combination of several methodologies, which are strongly dependent on the task to be solved. A distinction was therefore made between optimization and prediction tasks. Although almost all papers unanimously had the goal of setting up a team that would score as many points as possible, they came at the solution differently. The matrix distinguishes between publications that optimized only the team performance as a whole and those that predicted the performance for each individual player and then combined the best players into a team. At the same time, it investigated which papers relied on betting odds or another form of prediction markets. Lastly, the data used in each publication was analyzed. Due to the always different data, a generalized view was applied, which examines whether time-series data is used, whether the home advantage was taken into account and whether betting odds were used. \\
\indent The articles are sorted by criteria in the following order: \emph{'VBA Rank', 'Machine Learning Approach', 'Neural Network', 'Individual Performance', 'Betting Odds'}. This sorting ensures that the papers that are most similar to the thesis at hand and at the same time have a high VBA Rank are displayed first. For comparison purposes, the thesis at hand can be found at the bottom of the matrix. In this way, it can be quickly recognized that no publication deals precisely with the problem of the thesis. The paper that is closest to the topic is the paper by \citet{landers_machine_2017}, even if it investigates football instead of soccer.

% Concept-Matrix
\subimport{section/}{matrix.tex}

% conclude the concepts from the papers (3p)
The following paragraph summarises various concepts that have been frequently discussed in the presented literature. These topics are presented in alignment with the data mining process.  \\
\indent The first concept discussed is the \emph{preprocessing of the data}. In order to achieve optimal results, the data mining process must adjust the data in advance without compromising its validity. Many of the authors tackle the problem that few exceptional players outperform the average players. These outliers are firstly hard to predict and secondly degrade the prediction accuracy. To solve this problem, the authors used various techniques to boost their prediction accuracy. For example, \citet{landers_machine_2017} developed a calculated threshold that players must reach at least to be included in the analysis. All players below this threshold are sorted out. At this point, it should be mentioned that it is crucial to choose a threshold instead of a point range, as in this case, the players with the highest points are not omitted. This approach can only be applied if data from previous games are available. \citet{lutz_fantasy_2015,egidi_bayesian_2018,yurko_nflwar_2019} focus in their papers on what to do if this data is not available. \citet{yurko_nflwar_2019} state that one major problem they could not solve that negatively influences the team performance is the uncertainty of players appearing in the lineup due to unpredictable events with no evidential data like injuries. \citet{lutz_fantasy_2015} investigates the case of new players who joined at the beginning of the season ('new joiners'), as these players naturally do not have previous game data. He suggests taking the mean points of all players on a similar position in this case. \parencite[cf.][, p. 3]{lutz_fantasy_2015} In contrast to that, \citet{egidi_bayesian_2018} take a different approach. In their paper, they compare two different solutions to this problem. On the first try, they put the expected points to zero, and on the second try, they guessed the points in a calculated range. In both cases, the processed points from the player often were too low to be considered for their starting lineup. Nevertheless, they find out that the second approach is more precise and improves their models overall. Furthermore, \citet{egidi_bayesian_2018} discover that simplifying the data, if more details do not add value, increase their models' accuracy as well. This is similar to the approach \citet{deng_analysis_2020} take. In their studies, they use the \emph{Kaggle European Soccer Database}, a table with a total of 144 attributes, wherefrom they only carefully select 28 attributes to improve the model. \\
\indent This links to the second concept, the \emph{feature selection}. As mentioned, \citet{deng_analysis_2020,egidi_bayesian_2018} reduce the attributes fed to their model to increase accuracy. In his paper, \citet{lutz_fantasy_2015} examines precisely the question of if and how far the number of attributes must be limited. He proceeds in three different ways. First, he does not exclude any features, figuring out that this approach is the least accurate. Secondly, he selects the features manually according to his assessment. Lastly, he chooses a more analytical path: \emph{Recursive Feature Elimination with Cross Validation} (RFECV). This method \emph{'recursively eliminates features and checks if the regression method's results improve by cross-validating.'} \parencite[, p. 4]{lutz_fantasy_2015} This calculated approach yields the highest prediction accuracy. This method in combination with \emph{univariate selection} was used by \citet{anik_players_2018} as well. One key feature that is discovered in this way is the position of the players. \citet{lutz_fantasy_2015,demediuk_performance_2021,egidi_bayesian_2018} all increased their accuracy by modeling each position separately. Similar to \citeauthor{lutz_fantasy_2015}' second manual approach, \citeauthor{deng_analysis_2020} also select their features based on their perception and note that \emph{'sufficient background knowledge of the practical application is essential.'} \parencite[, p. 4]{deng_analysis_2020}. That confirms the discovery of \citeauthor{rein_big_2016}, who claims that at the current state of research, \emph{'most [Machine Learning] soccer analyses are performed by computer scientist research groups with little apparent involvement by sports scientists.'} \parencite[, p. 6]{rein_big_2016}. \\
\indent From these researches could be inferred that it is beneficial to interview sports experts on their opinion on essential features if manual feature selection is used. However, if this is not possible, feature selection algorithms should be applied. In addition, the models could be even further improved by omitting players and features that offer little added value for the predictions. Each position should thereby be modelled separately. Finally, missing data can be dealt with in three ways: setting it to zero, giving it a mean value from similar players, and estimating it accurately. The latter is promising the most success.

Once the feature selection process is complete, the next step, respectively the third concept, is to select the right \emph{machine learning approach}. First, as in the concept matrix, a distinction must be made between optimization and prediction tasks. Only two different methodologies were chosen for the optimisation task: either brute force optimisation \parencite{landers_machine_2017} or mixed-integer programming \parencite{becker_analytical_2016,edwards_analyzing_2018, belien_optimization_2017,bonomo_mathematical_2014,matthews_competing_2012}. Which of these two methods is used depends primarily on how much computing power is required for the previous task. \\
\indent For the prediction task, a variety of methods are used that range from simple linear regression to more complex feed-forward deep neural networks. In the following, the focus is limited to the three most frequently used and most promising methods:  \emph{Gradient Boosted Decision Trees (GBDT)} \parencite{landers_machine_2017,deng_analysis_2020}, \emph{Random Forest} \parencite{deng_analysis_2020,shah_poisson_2021,demediuk_performance_2021,bhateja_analysis_2021} and \emph{Deep Neural Networks (DNN)} \parencite{bhateja_analysis_2021,skinner_method_2015,deng_analysis_2020,lutz_fantasy_2015,landers_machine_2017}. In the paper of \citet{deng_analysis_2020}, all of the previously mentioned methods are used and compared. However, only a 'simple' Decision Tree model is used instead of a GBDT model. As a criterion for their Decision Tree, they use the 'information entropy', which is \emph{'a mathematical measure of the degree of randomness in a set of data, with greater randomness implying higher entropy and greater predictability implying lower entropy.'} \parencite[, p. 4]{deng_analysis_2020} They note that while Decision Tree models compute faster and require fewer data processing than Random Forest models, for this reason, they are more prone to over-fitting as the number of data increases, making them less accurate in general. The DNN they create consists of \emph{'5 fully-[connected] dense layers, five activation layers, two dropout layers and one batch normalization layer.'} \parencite[, p. 4]{deng_analysis_2020} They apply the \emph{Softmax} function to transform the data and use \emph{sparse categorical cross entropy} as base for the model's loss. The batch size is set to 32, and the model trains 500 epochs. According to their research regarding prediction accuracy, the DNN is the most accurate (0.99), followed by the Decision Tree model (0.91) and the Random Forest model (0.84), with the DNN probably being over-fitted with an accuracy of 0.99. \\
\citet{landers_machine_2017} use a GBDT model in their studies in which they want to predict the individual player performance for each player in the \emph{National Football League (NFL)}. They test the team their model predicts against 300.000 randomly selected teams and achieve the highest scores in five of eleven weeks. In addition, they let their model predict the 100 best team constellations and thereby manage to get into the profit range of the 20th percentile in 68\% of the cases. Unfortunately, they do not provide further information on their model but again emphasize the variety of well-thought and self-engineered features they use. Furthermore, like \citeauthor{deng_analysis_2020}, they also agree to the straightforward implementation of Decision Trees, as it is not necessary to normalize or scale the features. \parencite[cf.][, p. 6]{landers_machine_2017} \\
In the researches from \citet{shah_poisson_2021}, they attest the Random Forest model to produce the best results for their problem. They compare four different approaches to calculate the expected rate of goals. The calculations are based on: previous goals, expected goals from prediction markets, Linear Regression and Random Forest. To compare their models, they use the \emph{Brier Score}, \emph{'a score function that helps determine the accuracy of any probabilistic model. '} \parencite[, p. 7]{shah_poisson_2021} Another implementation of the Random Forest algorithm is used by \citet{demediuk_performance_2021}, who calculate a so-called \emph{'Performance Index (PI)'} for each player during a e-sport game of Dota2. Here, the algorithm is used to predict the chance of winning the game based on real-time in-game data. This, later on, helps the final calculation of the PI. Depending on the length of the game, the Random Forest model predicts the correct winner with an accuracy of 0.55 to 0.8. \\
Of all the methods used in the literature reviewed, DNNs are the most commonly used. Similar to \citet{deng_analysis_2020}, \citet{bhateja_analysis_2021} benchmark their feed-forward DNN against Machine Learning algorithms like K-Nearest Neighbours (KNN) or Random Forest. Although their DNN, with an accuracy between 0.88 and 0.94, does not appear to be over-fitted, it also outperforms all Machine Learning models by a margin of at least 0.08. Their DNNs input layer has one neuron for each feature fed to the classifier. \emph{'The model consisted of three hidden layers, each with 64, 32 and 16 neurons, respectively. Finally, the output layer consisted of 7 neurons [...]. A learning rate of 0.3 is used for training 500 epochs. A categorical cross-entropy loss function with sigmoid activation functions in hidden layers and a softmax activation function in the output layer is used for training the classifier. The basic hyperparameters [...] were empirically optimized using the grid search approach.'} \parencite[, p. 7]{bhateja_analysis_2021} In contrast to this more complex DNN, \citet{lutz_fantasy_2015} uses a DNN with only one hidden layer and compares it to his results with \emph{Support Vector Regression (SVR)}. The DNN with the best accuracy trained 50 epochs has 50 hidden units and uses the Sigmoid squashing function. This straightforward DNN already outperforms his SVR model slightly. In his conclusion, he states that DNNs with multiple hidden layers could provide increased accuracy. \parencite[cf.][, p. 5]{lutz_fantasy_2015} \\
\indent In summary, it can be concluded from the methods analyzed that there is no algorithm in this area of research that predominantly offers the best prediction accuracies. Instead, various methods must be experimented with and adapted precisely to the problem at hand. Nevertheless, the relevant literature shows methods that promise more success than others, which should be specially addressed for this reason. These methods include Decision Trees and Deep Neuronal Networks.

% betting odds
The final concept discussed in this chapter is the influence of \emph{betting odds} in current researches in this area. The impact of values that want to predict the future can already be observed in the literature reviewed. \citet{landers_machine_2017} for example, want to predict the winning team against the spread, based on historical spread betting data. \citet{shah_poisson_2021} use a metric called 'expected goals', which indicates how many goals a soccer team will score according to the participating bettors. Although \citet{deng_analysis_2020} do not explain any further how they use or process the betting odds in their dataset, they claim to feed them to their models. In his paper, \citet{wheatcroft_profiting_2020} investigates the overreaction of soccer betting odds in mismatch to the underlying reality. He, therefore, explains ubiquitous biases in sports betting, like the home-underdog bias and the contrary away-favourite bias. Although studies from \citet{nevill_home_1999} show that the home advantage does exist, many of the papers discuss how many influences the home advantage has. \parencite{bonomo_mathematical_2014, landers_machine_2017,shah_poisson_2021,deng_analysis_2020} In his studies, \citet{wheatcroft_profiting_2020} shows that there is a tendency to overestimate the influence of the home advantage. Furthermore, he defines a nominal statistic called \emph{'combined odds distribution'} (COD) which indicates \emph{'the performance relative to expectations of a team in its previous matches'} \parencite[, p. 4]{wheatcroft_profiting_2020}. If the COD is above 0.5, the team performed better than predicted by the odds and vice versa. From this statistic, he infers that the hot-hand bias exists in soccer bettings odds, where an event seems to have a higher probability if it occurred recently in the past. In addition, he explains that even though algorithms are generally considered not to be biased prone, they still are created by biased humans. \\
In their studies, \citet{spann_sports_2009} compare three different forecast methods, namely prediction markets, tipsters and betting odds. They discover that prediction markets and betting odds are more accurate than expert opinion. This discovery is in contrast to the results of \citet{goldstein_wisdom_2014}, who figured out that the prediction accuracy of smaller, smarter crowds tends to be higher than the general swarm intelligence. However, to obtain the highest prediction accuracy according to \citet{spann_sports_2009}, all three forecasting methods should be combined. \\
In the end, future predicting values such as spread bets, betting odds or prediction markets have already been used in the literature but not yet as planned in this work. Furthermore, it could be shown that almost every form of using these values promises general success. In addition, it was proven that even though these values are not free of biases, they still offer added value in forecasting.

% explain chosen gap / research question (1p)
In the following concluding paragraph of this chapter, as requested by \citet{webster_guest_2002}, the research gap found and the research question resulting from it will be addressed. Some of the authors in the presented literature state that \emph{'investigations into these [fantasy sports] games in the academic literature are virtually nonexistent'} \parencite[, p. 1]{landers_machine_2017} or \emph{'the prediction of Fantasy Football has barely been studied'} \parencite[, p. 1]{lutz_fantasy_2015}. \citet{rein_big_2016} even claim that all main characteristics of big data implementations are highly relevant and provide specific solutions to address tactical analytics in elite soccer. In addition to that, as already described in the previous paragraph, future predicting values, especially betting odds, seem to offer a significant value in increasing prediction accuracy. Although \citet{deng_analysis_2020} include betting odds in their studies, it is only one of many features, and they only aim to predict team performance instead of individual player performances. In contrast, this thesis closely observes this influence, taking the results of \citet{wheatcroft_profiting_2020} and \citet{goldstein_wisdom_2014} into account. \\
The central research question is first divided into two sub-questions. These questions are \emph{'How accurately can individual soccer player performances be predicted using historical data?'} and \emph{'How accurately can individual soccer player performances be predicted using betting odds?'} After these two questions are answered respectively; the central research question can be answered, which reads as follows:
\begin{quote}
    \centering
    \emph{'How accurately can individual soccer player performances be predicted using historical data and betting odds?'}
\end{quote}


