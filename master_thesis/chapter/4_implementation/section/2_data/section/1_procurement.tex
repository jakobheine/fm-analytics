\subsection{Procurement}

As mentioned in the previous section, two types of models will be compared: baseline and treatment models. While the baseline models only use the data provided by SPITCH, the treatment models additionally use betting odds from other sources. For this reason, the data needed for this project is divided into two groups: the SPITCH data and the betting odds. Furthermore, it is stated in the previous section that the final tool is divided into two phases: a batch processing offline phase and a stream processing online phase. These classifications end in four different sources needed to implement the tool: SPITCH data and betting odds for each of the online and offline stages.

The tool's architecture is designed after the microservice principles. The microservices are described throughout the \emph{Implementation} chapters, and the final architecture is again summarised in the last section of this chapter. The tool and with it each associated microservice is launched on a virtual private server (VPS) hosted on \emph{Hetzner}. \parencite[see][]{hetzner_about_2021} The core of the architecture is the PostgreSQL \emph{Database} microservice, which stores all of the data mentioned in the following paragraphs.

\subsubsection{SPITCH Data}

The SPITCH data derives from the same source regardless of the ongoing tool's phase. The data is crawled directly from their API-endpoint \underline{https://api.spitch.live} from the \emph{Crawler} microservice. The \emph{Crawler} therefore executes a script in a manually definable schedule. Additionally, the \emph{Proxy} microservice runs in the background. The requests sent from the \emph{Crawler} are proxied via this service. The \emph{Proxy} then routes the request via a dynamic number of different IP addresses (exit nodes) within the \emph{Tor} network. This procedure bypasses the rate limits of the recipient's firewall, as the requests can no longer be clearly assigned to a single IP address, allowing the \emph{Crawler} to send multiple requests per second. However, with programming ethics in mind, the requests per second still were set to a relatively small number to avoid over floating the endpoint.

SPITCH data refers to player-specific and event-based data. Player-specific data includes tables such as the \emph{Player} table with information about their position, transfer market value, names, and team membership. The player data was crawled from the endpoint \underline{https://api.spitch.live/\textbf{contestants}}. This endpoint sends all players and teams from the SPITCH database in the \emph{JavaScript Object Notation} (JSON) format. The JSON is received, converted, and stored in two separate tables \emph{Player} and \emph{Team} in the own \emph{Database}. This procedure took place once in the offline phase and takes place before each matchday in the online phase. The latter ensures that the player and team data is up to date for the model's predictions. During the conversion of the request, the transfer market value of each player is stored in a different table \emph{Market Value} with an additional period of validity. In data management or warehousing terms, the table \emph{Market Value} is implemented as \emph{Slowly Changing Dimension Type 2}. This modification was made because the transfer market value is a feature of the players that changes frequently but whose history is helpful for later investigations. For this reason, information would be lost if the transfer market value were merely overwritten on update. 

However, the core data is event-specific, stored in the \emph{Event} table. Each event on the pitch during a match is stored in this table, represented by one row. Therefore, one row in the table interprets like: '\emph{Player A has performed event B in minute C on matchday D.},' i.e. '\emph{Manuel Neuer played a pass in the 35th minute of the 6th matchday.}' This data can only be gathered matchday- and player-specific, using the corresponding endpoints following the structure:\\
\centerline{\underline{https://api.spitch.live/matchdays/\textbf{matchday\_id}/players/\textbf{player\_id}/events}}

Consequently, the \emph{Crawler} first has to get all the player and matchday identifiers (id) to gather all the data provided by these endpoints. The tool is able to get all the player identifiers using the method described above in the player paragraph. Simultaneously, the \emph{Crawler} has to get all the matchday identifiers by requesting the endpoint \underline{https://api.spitch.live/matchdays} and storing the received information in the table \emph{Matchday}. If player and matchday identifiers are provided, the event data can be queried. The service then takes two extra checks before starting the crawling. First, it checks for the latest matchday in the \emph{Event} table to prevent duplicate queries. This step is particularly essential in the online phase, as the tool is executed regularly on different occasions. Second, it proves which of the matchdays in the table \emph{Matchday} have already occurred by taking the current timestamp and comparing it with the timestamps of the matchdays. This checkup ensures that no endpoints from matchdays that lay in the future are getting requested, leading to empty responses. After these two checks are made, the \emph{Crawler} first gets and then stores all the available event data in the \emph{Database}.

It is important to notice that SPITCH most likely uses an official API as well to gather this information, like \underline{https://www.api-football.com/} for instance. For further studies, it would be beneficial to investigate to what extend the data from SPITCH matches with the data from such APIs since they are easier to request and provide much more data, which reaches far into the past. At the same time, however, it must be noted that more data could also lead to less accurate results, as the development of individual players over the years represents an additional and very complex influence. Only the connection between player data and betting odds could be examined more closely. In both cases, the amount of data now available is likely to be sufficient.

The following table serves as an overview of the various SPITCH-based tables in the \emph{Database}, explaining how many records are included and how much memory they occupy.

\subsubsection{Betting Odds}

The betting odds data in this thesis are obtained in two different ways. Here, a distinction is made between the online and offline phases. In the offline phase, a comma-separated values (CSV) file is taken from the following website: \\\underline{https://www.football-data.co.uk/germanym.php}. In this CSV file are all betting odds for each game of a season for different betting odds providers. Furthermore, columns like the average betting odds from all providers are added. In the online phase, the data is taken from this API: \underline{https://the-odds-api.com/}. In this phase, care is taken to request the odds as late as possible, as they often change before the start of the matchday given to the most recent information. Here, no average betting odds column is provided and therefore needs to be calculated itself by the \emph{Crawler} microservice. This data gets stored in the \emph{Odds} Table of the \emph{Database}. Each row thereby represents the winning, draw, and losing odds for a team for a matchday.

