{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Install python dependencies**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install -q -r ./dependencies/requirements.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Load python libraries**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sqlalchemy import create_engine\n",
    "from os import getenv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import smart_match\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Connect to database**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "db_name = getenv(\"POSTGRES_DB\")\n",
    "db_user = getenv(\"POSTGRES_USER\")\n",
    "db_pass = getenv(\"POSTGRES_PASSWORD\")\n",
    "db_host = 'localhost'\n",
    "db_port = '5432'\n",
    "db_string = 'postgresql://{}:{}@{}:{}/{}'.format(db_user, db_pass, db_host, db_port, db_name)\n",
    "db = create_engine(db_string)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Preprocess Data: Event Data**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# getting event data from database\n",
    "db_response = db.execute(\n",
    "    f\"\"\"select  (coalesce(p.first_name, '') || ' ' ||coalesce(p.last_name, '')) as name,\n",
    "       position,\n",
    "       club_id,\n",
    "       number as matchday,\n",
    "       type as event_type,\n",
    "       count(*) from events\n",
    "        inner join matchdays m on events.matchday_id = m.id\n",
    "        inner join players p on events.player_id = p.id\n",
    "    where corrected is false and p.active is true\n",
    "    group by name, position, matchday, event_type, club_id\n",
    "    order by name, matchday,  event_type\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "df_event = pd.DataFrame(db_response.fetchall())\n",
    "df_event.columns = db_response.keys()\n",
    "\n",
    "df_event"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load event_types\n",
    "event_types = ['pass', 'unsuccessfulPass', 'superPass', 'throwIn', 'goalAssist', 'farCorner', 'cross', 'freeKick', 'interception', 'goalMissedFar', 'shotAtGoal', 'unsuccessfulPenalty', 'missedChance', 'goal', 'doublePack', 'hattrick', 'penaltyGoal', 'ownGoal', 'foul','awardedPenalty', 'causedPenalty', 'yellowCard', 'secondYellowCard', 'redCard', 'successfulTackle', 'unsuccessfulTackle', 'blockedGoalShot', 'offside', 'error', 'savedPenalty', 'lostPenalty', 'defended', 'goalAgainst']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_players = df_event.groupby('name')\n",
    "\n",
    "df_final = pd.DataFrame(columns=['name','position','matchday','event_type','count', 'club_id'])\n",
    "\n",
    "# add 0 values\n",
    "for player_tuple in df_players:\n",
    "    player_name = player_tuple[0]\n",
    "    position = player_tuple[1]['position'].max()\n",
    "    club_id = player_tuple[1]['club_id'].max()\n",
    "\n",
    "    df_player = player_tuple[1]\n",
    "\n",
    "    for matchday in range(1,35):\n",
    "        for event_type in event_types:\n",
    "            if df_player.loc[(df_player['matchday'] == matchday) & (df_player['event_type'] == event_type)].empty == True:\n",
    "                df_player = df_player.append({\n",
    "                    'name': player_name, \n",
    "                    'position': position,\n",
    "                    'club_id': club_id,\n",
    "                    'matchday': matchday,\n",
    "                    'event_type': event_type,\n",
    "                    'count': 0\n",
    "                }, ignore_index=True)\n",
    "    \n",
    "    df_final = df_final.append(df_player)\n",
    "\n",
    "df_final.to_csv('./data/events_prep2.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Preprocess Data: Betting Odds**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load odds from csv\n",
    "df_odds_csv = pd.read_csv('./data/odds_20_21.csv')\n",
    "df_odds = df_odds_csv[['Date', 'HomeTeam', 'AwayTeam', 'AvgH', 'AvgD', 'AvgA']]\n",
    "df_odds['Date'] = df_odds['Date'].map(lambda x: datetime.strptime(datetime.strptime(x, \"%d/%m/%Y\").strftime(\"%Y-%m-%d\"), \"%Y-%m-%d\"))\n",
    "df_odds['matchday'] = 0\n",
    "\n",
    "# add matchday to odds\n",
    "df_matchdays_db = pd.read_sql_table('matchdays', db)\n",
    "df_matchdays = df_matchdays_db[['start', 'end', 'number']]\n",
    "df_matchdays['start'] = df_matchdays['start'].map(lambda x: datetime.strptime(x.split('T')[0], \"%Y-%m-%d\"))\n",
    "df_matchdays['end'] = df_matchdays['end'].map(lambda x: datetime.strptime(x.split('T')[0], \"%Y-%m-%d\"))\n",
    "\n",
    "for index, row in df_odds.iterrows():\n",
    "    for index2, row2 in df_matchdays.iterrows():\n",
    "        if row2['start'] <= row['Date'] <= row2['end']:\n",
    "            df_odds.loc[index, 'matchday'] = row2['number']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# game odds to win, lose, draw odds per team\n",
    "df_odds_team = pd.DataFrame(columns=['team_name','matchday', 'is_home', 'odds_win','odds_draw','odds_lose'])\n",
    "\n",
    "for index, row in df_odds.iterrows():\n",
    "    df_odds_team = df_odds_team.append({\n",
    "        'team_name': row['HomeTeam'],\n",
    "        'matchday': row['matchday'],\n",
    "        'is_home': True,\n",
    "        'odds_win': row['AvgH'],\n",
    "        'odds_draw': row['AvgD'],\n",
    "        'odds_lose': row['AvgA'],\n",
    "    } , ignore_index=True)\n",
    "\n",
    "    df_odds_team = df_odds_team.append({\n",
    "        'team_name': row['AwayTeam'],\n",
    "        'matchday': row['matchday'],\n",
    "        'is_home': False,\n",
    "        'odds_win': row['AvgA'],\n",
    "        'odds_draw': row['AvgD'],\n",
    "        'odds_lose': row['AvgH'],\n",
    "    } , ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# odds team names to club_ids\n",
    "df_teams = pd.read_sql_table('teams', db)\n",
    "\n",
    "# default is levenstein, which mixes Augsburg and Freiburg\n",
    "smart_match.use('Smith Waterman')\n",
    "\n",
    "def name_to_club_id(name): \n",
    "    most_similar = {'db_club_id': '', 'similarity': 0 }\n",
    "    for index, db_team in df_teams.iterrows():\n",
    "        similarity = smart_match.similarity(db_team['name'], name)\n",
    "        if most_similar['similarity'] < similarity:\n",
    "            most_similar['db_club_id'] = db_team['id']\n",
    "            most_similar['similarity'] = similarity\n",
    "            \n",
    "    return most_similar['db_club_id']\n",
    "\n",
    "df_odds_team['club_id'] = df_odds_team['team_name'].map(lambda x: name_to_club_id(x))\n",
    "\n",
    "df_odds_team = df_odds_team.sort_values(by=['matchday', 'club_id'])\n",
    "\n",
    "df_odds_team.to_csv('./data/odds_prep.csv', index=False)\n",
    "\n",
    "# !!! MANUALLY CHANGE COVID SHIFTED MATCHES !!!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DEBUGGING CELL\n",
    "\n",
    "# df_teams = pd.read_sql_table('teams', db)\n",
    "\n",
    "# smart_match.use('Smith Waterman')\n",
    "\n",
    "# df_similarities = pd.DataFrame(columns=['odd_name','db_name', 'similarity'])\n",
    "\n",
    "# for team_tuple in df_odds_team.groupby(\"team_name\"):\n",
    "#     odd_name = team_tuple[0]\n",
    "    \n",
    "#     for index, row in df_teams.iterrows():\n",
    "#         db_name = row['name']\n",
    "#         df_similarities = df_similarities.append({'odd_name': odd_name, 'db_name': db_name, 'similarity': smart_match.similarity(odd_name, db_name)}, ignore_index=True)\n",
    "\n",
    "# # df_similarities.to_csv('./data/similarities.csv')\n",
    "\n",
    "# df_similarities[df_similarities.groupby(\"odd_name\")['similarity'].transform(max) == df_similarities['similarity']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Merge Event data and Betting Odds**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_events = pd.read_csv('./data/events_prep.csv')\n",
    "df_odds = pd.read_csv('./data/odds_prep.csv')\n",
    "\n",
    "df = pd.merge(df_events, df_odds)\n",
    "\n",
    "# remove playeron and playeroff\n",
    "df = df.drop(df[df.event_type == 'playerOn'].index)\n",
    "df = df.drop(df[df.event_type == 'playerOff'].index)\n",
    "\n",
    "# rename count to occurences\n",
    "df = df.rename(columns={'count':'occurences'})\n",
    "\n",
    "df.to_csv('./data/data.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Create Label/Target Column**\n",
    "\n",
    "The label for each row is the upcoming value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('./data/data.csv')\n",
    "\n",
    "df_label = pd.DataFrame()\n",
    "\n",
    "for event_type, df_e in tqdm(df.groupby('event_type')):\n",
    "    for player_name, df_e_p in df_e.groupby('name'):\n",
    "        \n",
    "        df_e_p = df_e_p.sort_values('matchday')\n",
    "\n",
    "        # Add weighted occurences\n",
    "        df_e_p.set_index('matchday', inplace=True)\n",
    "        df_e_p['weighted_occurences'] = df_e_p.occurences.ewm(alpha=0.5, adjust=False).mean().map(lambda x: int(x))\n",
    "\n",
    "        # Add label\n",
    "        df_e_p['label'] = df_e_p['occurences'].shift(periods=-1, fill_value=0)\n",
    "\n",
    "        df_label = df_label.append(df_e_p)\n",
    "\n",
    "df_label.to_csv('./data/data_labeled.csv')\n",
    "\n",
    "df_label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Split dataframe into input and label; transform categorical values into numbers**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "df_label = pd.read_csv('./data/data_labeled.csv')\n",
    "\n",
    "label = df_label['label']\n",
    "df_inputs = df_label.drop('label', axis='columns')\n",
    "\n",
    "df_inputs['team_name_normalized'] = LabelEncoder().fit_transform(df_inputs['team_name'])\n",
    "df_inputs['event_type_normalized'] = LabelEncoder().fit_transform(df_inputs['event_type'])\n",
    "df_inputs['position_normalized'] = LabelEncoder().fit_transform(df_inputs['position'])\n",
    "\n",
    "df_inputs_n = df_inputs.drop(['team_name', 'event_type', 'position', 'club_id', 'occurences', 'name'], axis='columns')\n",
    "\n",
    "label.to_csv('./data/label.csv')\n",
    "df_inputs_n.to_csv('./data/inputs.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "metadata": {
   "interpreter": {
    "hash": "74907149d8cf8355762064ff20aca90cf6505446beaf5da5d3df333eff494a69"
   }
  },
  "interpreter": {
   "hash": "74907149d8cf8355762064ff20aca90cf6505446beaf5da5d3df333eff494a69"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}