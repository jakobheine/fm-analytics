{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare Workbook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install python dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip install -q -r ./dependencies/requirements.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load python libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import svm\n",
    "\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = pd.read_csv('./data/final_score.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add prev_score coloumn\n",
    "\n",
    "Calculated after research in **smoothing_comparison.ipynb**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df_prev_score = pd.DataFrame()\n",
    "\n",
    "for player, df_player in df.groupby(['name']):\n",
    "    \n",
    "    df_player = df_player.sort_values('matchday')\n",
    "    df_player.set_index('matchday')\n",
    "    df_player['prev_score_smoothed'] = df_player.final_score.ewm(alpha=0.5, adjust=False).mean().map(lambda x: int(x)).shift(periods=1, fill_value=0)\n",
    "    df_player['prev_score'] = df_player.final_score.shift(periods=1, fill_value=0)\n",
    "\n",
    "    df_prev_score = df_prev_score.append(df_player, ignore_index=True)\n",
    "\n",
    "df = df_prev_score\n",
    "\n",
    "df['guessed_score'] = [ random.randint(df['final_score'].min(),df['final_score'].max())  for k in df.index]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Min-Max-Scaling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "scaled_columns = ['prev_score', 'prev_score_smoothed', 'guessed_score', 'odds_win', 'odds_draw', 'odds_lose']\n",
    "df_scaled = pd.DataFrame(MinMaxScaler().fit_transform(df[scaled_columns]), columns=scaled_columns)\n",
    "df = df.drop(scaled_columns, axis='columns').join(df_scaled)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-Hot-Encoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "one_hot_columns = ['club_id', 'position']\n",
    "df = pd.get_dummies(data=df, columns=one_hot_columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate accuracies for different models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load helper methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def calculate_best_lineup(df, score_column):\n",
    "    possible_lineups = [[3,4,3], [3,5,2], [4,2,4], [4,3,3], [4,4,2], [4,5,1], [5,3,2], [5,4,1], [5,2,3], [3,3,4]]\n",
    "\n",
    "    best_lineup = pd.DataFrame({score_column: 0}, index=[0])\n",
    "\n",
    "    for number_of_defender, number_of_midfielder, number_of_attacker in possible_lineups:\n",
    "        df_goalkeeper = df.loc[df['position_goalkeeper'] == True].nlargest(1, score_column, keep='first')\n",
    "        df_defender = df.loc[df['position_defender'] == True].nlargest(number_of_defender, score_column, keep='first')\n",
    "        df_midfielder = df.loc[df['position_midfielder'] == True].nlargest(number_of_midfielder, score_column, keep='first')\n",
    "        df_attacker = df.loc[df['position_attacker'] == True].nlargest(number_of_attacker, score_column, keep='first')\n",
    "\n",
    "        df_lineup = pd.concat([df_goalkeeper, df_defender, df_midfielder, df_attacker])\n",
    "\n",
    "        captain_id = df_lineup['final_score'].idxmax()\n",
    "        captain_score = df_lineup.at[captain_id, 'final_score']\n",
    "        df_lineup.at[captain_id, 'final_score'] = captain_score * 2    \n",
    "\n",
    "        if df_lineup[score_column].sum() > best_lineup[score_column].sum():\n",
    "            best_lineup = df_lineup\n",
    "\n",
    "    return best_lineup\n",
    "\n",
    "def calculate_lineup_accuracies(df, column):\n",
    "    df_results = pd.DataFrame()\n",
    "\n",
    "    for matchday, df_matchday in df.groupby('matchday'):\n",
    "        df_predicted_lineup = calculate_best_lineup(df_matchday, column)\n",
    "        df_best_lineup = calculate_best_lineup(df_matchday, 'final_score')\n",
    "\n",
    "        predicted_lineup_total_score = df_predicted_lineup['final_score'].sum()\n",
    "        best_lineup_total_score = df_best_lineup['final_score'].sum()\n",
    "\n",
    "        df_results = df_results.append({'Matchday': matchday, 'Predicted': predicted_lineup_total_score, 'Best': best_lineup_total_score }, ignore_index=True)\n",
    "\n",
    "    df_results = df_results[['Matchday', 'Predicted', 'Best']]\n",
    "\n",
    "    df_results['Difference'] = df_results['Best'] - df_results['Predicted']\n",
    "    df_results['points_in_%'] = round(df_results['Predicted'] / df_results['Best'],2)\n",
    "\n",
    "    return {'MAE_Lineup': df_results['Difference'].mean(), 'Std_Lineup': df_results['Difference'].std(), 'Mean_%_from_Best_Lineup': df_results['points_in_%'].mean(), 'Std_%_from_Best_Lineup': df_results['points_in_%'].std()}\n",
    "\n",
    "\n",
    "def calculate_regression_accuracies(y, yhat):\n",
    "    mae = metrics.mean_absolute_error(y, yhat)\n",
    "    mse = metrics.mean_squared_error(y, yhat)\t\n",
    "    rmse = np.sqrt(mse) \n",
    "    r2 = metrics.r2_score(y, yhat)\n",
    "    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "def get_models():\n",
    "    linr = LinearRegression() # linear regression model\n",
    "    logr = LogisticRegression() # logistic regression model\n",
    "    dt = DecisionTreeRegressor() # decision tree model\n",
    "    rf = RandomForestRegressor() # random forest model\n",
    "    kn = KNeighborsRegressor() # k-nearest neighbours model\n",
    "    sv = svm.SVC() # support vector machine model\n",
    "    return [linr, logr, dt, rf, kn, sv]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "df_test = df[df['matchday'] > 28]\n",
    "\n",
    "df_model_accuracies = pd.DataFrame()\n",
    "\n",
    "regression_accuracies = calculate_regression_accuracies(df_test['final_score'], df_test['prev_score'])\n",
    "lineup_accuracies = calculate_lineup_accuracies(df_test, 'prev_score')\n",
    "accuracies = {**regression_accuracies, **lineup_accuracies}\n",
    "accuracies['model'] = 'prev_score'\n",
    "df_model_accuracies = df_model_accuracies.append(accuracies, ignore_index=True)\n",
    "\n",
    "regression_accuracies = calculate_regression_accuracies(df_test['final_score'], df_test['prev_score_smoothed'])\n",
    "lineup_accuracies = calculate_lineup_accuracies(df_test, 'prev_score_smoothed')\n",
    "accuracies = {**regression_accuracies, **lineup_accuracies}\n",
    "accuracies['model'] = 'prev_score_smoothed'\n",
    "df_model_accuracies = df_model_accuracies.append(accuracies, ignore_index=True)\n",
    "\n",
    "regression_accuracies = calculate_regression_accuracies(df_test['final_score'], df_test['guessed_score'])\n",
    "lineup_accuracies = calculate_lineup_accuracies(df_test, 'guessed_score')\n",
    "accuracies = {**regression_accuracies, **lineup_accuracies}\n",
    "accuracies['model'] = 'guessed_score'\n",
    "df_model_accuracies = df_model_accuracies.append(accuracies, ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compare model accuracies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df_model_accuracies[['model', 'Mean_%_from_Best_Lineup', 'Std_%_from_Best_Lineup', 'MAE_Lineup', 'Std_Lineup', 'MAE', 'R2']].sort_values('Mean_%_from_Best_Lineup', ascending=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Mean_%_from_Best_Lineup</th>\n",
       "      <th>Std_%_from_Best_Lineup</th>\n",
       "      <th>MAE_Lineup</th>\n",
       "      <th>Std_Lineup</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prev_score_smoothed</td>\n",
       "      <td>0.511667</td>\n",
       "      <td>0.131972</td>\n",
       "      <td>3391.833333</td>\n",
       "      <td>868.321235</td>\n",
       "      <td>183.378721</td>\n",
       "      <td>-1.455029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prev_score</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.151745</td>\n",
       "      <td>3872.166667</td>\n",
       "      <td>1289.940993</td>\n",
       "      <td>183.424827</td>\n",
       "      <td>-1.456041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>guessed_score</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.111355</td>\n",
       "      <td>4525.833333</td>\n",
       "      <td>962.586602</td>\n",
       "      <td>183.177352</td>\n",
       "      <td>-1.451708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  Mean_%_from_Best_Lineup  Std_%_from_Best_Lineup  \\\n",
       "1  prev_score_smoothed                 0.511667                0.131972   \n",
       "0           prev_score                 0.453333                0.151745   \n",
       "2        guessed_score                 0.350000                0.111355   \n",
       "\n",
       "    MAE_Lineup   Std_Lineup         MAE        R2  \n",
       "1  3391.833333   868.321235  183.378721 -1.455029  \n",
       "0  3872.166667  1289.940993  183.424827 -1.456041  \n",
       "2  4525.833333   962.586602  183.177352 -1.451708  "
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74907149d8cf8355762064ff20aca90cf6505446beaf5da5d3df333eff494a69"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}